# While a program runs {#intro}

- Processor fetches an instruction from memory

- Processor decodes (figures out which instruction this is)

- Processor executes (does its thing: add two numbers, access mem, check condition, etc.)

- Basics of Von Neumann model;

## Operating System {#OS}

- A body of software that is responsible for making programs easy to run many at a time, allows programs to share memory, interact with devices, and is in charge of making sure the system operates correctly and efficiently in an easy-to-use-manner  

## Virtualization {#virtualization}

- The primary method used by operating systems to make them easy to use

- Using physical resources ( the processor, memory, disk) and transforms it into more general, powerful and easy to use **virtual** form of itself  

- Thus, OS are sometimes referred to as a **virtual machine**

## System Calls {#system-calls}

- In order to allow users to tell the OS what to do, thus making use of the virtual machine(i.e. running a program, accessing a file),
the OS provices APIs that one can call

- A typical OS exports a few hundred **system calls** that are available to applications.

- OS provides these calls to run programs, access memory, and devices: it is said then that the OS provides a **standard library** to applications.

## OS As Resource Manager {#resource-manager}

- Because virtualization allows multiple programs to run thus sharing memory, the OS is sometimes known as a **resource manager**

- The resources it manages are the CPU, memory, and disk

- The Os's role is to **manage** these resources

### Figure 2.1: Code That Loops and Prints {#figure-2.1}

```#include <stdio.h>
#include <stdlib.h>
#include <sys/time.h>
#include <assert.h>
#include "common.h"

int
main(int argc, char *argv[])
{
   if (argc != 2) {
      fprintf(stderr, "usage: cpu <string>\n");
      exit(1);
   }
   char *str = argv[1];
   while (1) {
      Spin(1);
      printf("%s\n", str);
   }
   return 0;
}
```

### 2.1 Virtualizing The CPU {#Virtualizing-the-cpu}

- The previous figure renders a call to `Spin()`, which routinely checks the time and returns every one second, and prints out.

- Suppose the file is saved to `cpu.c` along with `common.h`, the illusion of running multiple programs on one single processor can be done by using ampersand in between commands such as

```prompt> ./cpu A & ./cpu B & ./cpu C
```

- The operating system is in charge of this **illusion** i.e., the illusion that the system has a very large number of virtual CPUs.

- __Turning a single CPU (or small set of them) into a seemingly infinite number of CPUs and thus allowing many programs to **_seemingly_** run at once is called Virtualizing the CPU__

## 2.2 Virtualizing Memory {#virtualizing-memory}

- Memory is just an array of bytes; to read memory, one must specify an address to be able to access the data stored there

- To write (update) memory, one must also specify the data to be written to the given address

- Memory is accessed all the time during the run of a program

- Programs keep all of their data structures in memory, and access them through various instructions, like loads and stores or other isntructions that access memory

- Each instruction of the program is in memory as well; thus memory is accessed on each instruction fetch

### Figure 2.3: A Program That Accesses Memory {#figure-2.3}

```#include <unistd.h>
#include <stdio.h>
#include <stdlib.h>
#include "common.h"

int
main(int argc, char *argv[])
{
   int *p = mallac(sizeof(int));
   assert(p != NULL);
   printf((%d) address pointed to be p: %p\n", getpid(), p);
   *p = 0;
   while (1) {
      Spin(1);
      *p = *p + 1;
      printf("(%d) p: %d\n", getpid(), *p);
   }
   return 0;
}
```

- This program allocates some memory by calling `malloc()`.

- Prints out the address of the memory and puts the number `0` into the first slot of the newly allocated memory

- Finally, it loops, delaying for a second and incrementing the value stored at the address held in `p`.

- The PID is unique per running process

### Figure 2.4: Running a Memory Program Multiple Times {#figure-2.4}

```prompt> ./mem &; ./mem &
[1] 24113
[2] 24114
(24113) address pointed to by p: 0x200000
(24114) address pointed to by p: 0x200000
(24113) p: 1
(24114) p: 1
(24114) p: 2
(24113) p: 2
(24113) p: 3
(24114) p: 3
(24113) p: 4
(24114) p: 4
...
```

- Each running program has allocated memory at the same address (`0x200000`), and yet, each seems to be updating the value at `0x200000` indeoendently!

- It is as if each running program has its own provate memory, instead of sharing the same physical memory with other running programs. [^5]

- The OS is **virtualizing memory**

- Each process accesses its own provate **virtual address space** or **address space**, which the OS **somehow** maps onto the physical memory of the machine

- A memory reference within one running program does not affest the address space of other processes (Or the OS itself)

- The running program has physical memory all to itself

- Physical memory is a shared resource managed by the OS

[^5]: For this to work, you need to make sure address-space randomization is disabled; randomization can be a good defense against certin kids of security flaws.  Use this to learn how to break into computer systems via stack-smashing attacks.

## 2.3 Concurrency {#concurrency}

- This concept is used to solve the many issues that arrise when working on many things at once (concurrently) in the same program

- The OS is juggling many things at once, first running once process, then another, and so forth; leading into some _deep_ and interesting problems.

### Figure 2.5: A Multi-threaded Program (threads.c) {#figure-2.5}

```#include <stdio.h>
#include <stdlib.h>
#include "common.h"

volatile int counter = 0;
int loops;

void *worker(void *arg) {
   int i;
   for (i = 0; i < loops; i++) {
      counter ++;
   }
   return NULL;
}

int
main(int argc, char *argv[])
{
   if (argc != 2) {
      fprintf(stderr, "usage: threads <value>\n");
      exit(1);
   }
   loops = atoi(argv[1]);
   pthread_t p1, p2;
   printf("Initial value: %d\n", counter);

   Pthread_create(&p1, NULL, worker, NULL);
   Pthread_create(&p2, NULL, worker, NULL);
   Pthread_join(p1, NULL);
   Pthread_join(p2, NULL);
   printf("Final value : %d\n", counter);
   return 0;
}
```

- The `main` program creates two **threads** using `Pthread_create()`

- You can think og a thread as a function running within the same memory space as other functions, with more than one of them active at a time.

- In this example, each thread starts running in a routing called `worker()`, in which it simply increments a counter in a loop for `loops` number of time.

- The value of `loops` determines how many times each of the two workers will incremenet the shared counter in a loop

- When the input value of `loops` is set to _N_, we would expect the final output for the program to be _2N_

- When using high numbers, you will get different values; relating to how the instructions are executed: which is one at a time. 

- Three instructions are taking place:

1. Load the value of the counter from memory into a register

2. Increment the value

3. Store the value back into memory

- Because these 3 instructions fo not execute **atomically** (all at once), strange things can happen.

- It is the problem of **concurrency**

## 2.4 Persistence {#persistence}

- The third major theme of this course is **persistence**

- In system memory, data can be easily lost, as devices such as DRAM store values in a **volatile** manner; such as power outtage

- The hardware to store data **persistently** is **input/output** or **I/O**

- Hard drives are a common repository for long-lived information, although SSD's are making headway in this arena 

- The software in the OS that usuallu manages the disk is called the **file system**; and thus responsible for storing any **files** that the user creates in a reliable and efficient manner on the disks of the system.

- The OS does not create a private, virtualized disk for each application

- First, VIM creates a file that serves as input to the compiler

```gcc -o main main.c```

- The cmopiler uses that input file to create a new executable file (in many steps)

- Finally, the new executable is done, a new program is born!

- To understand this better, **Figure.26** presents code to create a file `/tmp/file` that contains the string "hello world"

### Figure 2.6: A Program That Does I/O (io.c) {#figure-2.6}

```#include <stdio.h>
#include <unistd.h>
#include <assert.h>
#include <fcntl.h>
#include <sys/types.h>

int
main(int argc, char *argv[])
{
   int fd = open("/tmp/file", O_WRONLY | O_CREAT | O_TRUNC, S_IRWXU);
   assert(fd > -1);
   int rc = write(fd, "hello world\n", 13);
   assert(rc == 13);
   close(fd);
   return 0;
}
```

---

### THE CRUX OF THE PROBLEM: HOW TO STORE DATA PERSISTENTLY {#crux-persistence}

- The file system is the part of the OS in charge of managing persistent data.  

- What techniques are needed to do so correctly?

- What mechanisms and policies are required to do so with high performance?

- How is reliability achieved, in the face of failures in hardware and software?

---

- To accomplish this task, the program makes three calls into the OS

- The first call to `open()`, opens the files and creates it

- The second, `write()`, writes some data to the file

- The third, `close()`, closes the file - this indicating the program won't be writing any more data to it.

- These **system calls** are routed to the part of the OS called the **file system**, which then handles the requests and returns some kind of error code to the user.

- How does the OS write to a disk?

1. First, the OS figures out _where_ on the diskthis new data will reside

2. Keeps track of the location in various structures the file system maintains. Which requires issuing I/O requests to the underlying storage device, to either read existing structures or update them

- Getting a device to do something takes deep knowledge of the low-level device interface and its exact semantics.  

- Fortunately, the OS provides a standard and simple way to access devices through its system calls

- Thus, the OS is sometimes seen as a **standard library**

- For performance reasons most files first delat such writes for a while, hoping to batch them into larger groups

- To handle the problems of system crashes during write protocol, such as **journalizing** or **copy-on-write**, carefully ordering writes to disk to ensure that if a failure occurs during the write sequence, the system can recover to reasonable state afterwards

- To ake different common operations efficient, file systems employ a multitude of data structures and **_access methods_** from simple lists to complex b-trees.

## 2.5 Design Goals

- To summarize, an OS takes physical **resources** such as a CPU, memory, or disk, and **virtualizes** them

- It handles tough and tricky issues related to **concurrency**

- The stores files **persistently**, thus making them safe over a long-term

- Given that we want to build such a system, we need #GOALS

- Finding the right right set of trade-offs is a key to building systems.

1. Build up some **Abstraction**

- One of the most basic goal is to build up some **abstraction** in order to make the system convenient and easy to use.

- Abstractions are fundamental to everything we do in computer science

- Abstraction makes it possible to write a large program by dividing it into small and understandable pieces, to write such a program in a high-level language like `C` without thinking about assembly

--

1. Build up some **Abstraction**

2. Provide high **Performance** or **Minimize Overheads**

- Virtualization and making the system easy to use are well worth it, but not at any cost; thus we must strive to provide virtualization and other OS features without excessive overheads.

- These overheads arise in a number of forms: _extra time_ (more instructions), and _extra space_ (in memory or on disk).

- Perfection is not always attainable, something we will learn to notice and tolerate where necessary.

--

1. Build up some **Abstraction**

2. Provide high **Performance** or **Minimize Overheads**

3. Provide **Protection** between applications, as well as between the OS and applications

- Because we wish to allow many programs run at the same time, we want to make sure that the malicious or accidental bad behavior of one does not harm others

- We certainly don't want an application to harm the OS

- Protection is as heart **_one of the main principles underlying an operating system_** which is that of **Isolation**

- **Isolation** processes from one another is the key to protection and this underlies much of what an OS must do

--

1. Build up some **Abstraction**

2. Provide high **Performance** or **Minimize Overheads**

3. Provide **Protection** between applications, as well as between the OS and applications

4. Provide a high degree of **Reliability**

- The OS must also run non-stop; when it fails, _all_ applications running on the system fail as well; thus must often strive to provide high degree of **reliability**

- This is quite a challenge with an OS being millions of lines of code

--

1. Build up some **Abstraction**

2. Provide high **Performance** or **Minimize Overheads**

3. Provide **Protection** between applications, as well as between the OS and applications

4. Provide a high degree of **Reliability**

5. **Energy-Efficiency**

- Is important in our increase green world.

--

1. Build up some **Abstraction**

2. Provide high **Performance** or **Minimize Overheads**

3. Provide **Protection** between applications, as well as between the OS and applications

4. Provide a high degree of **Reliability**

5. **Energy-Efficiency**

6. **Security**

- An extension of protection, really

- Protect against malicious applications is critical, especially in these highly-networked times.

--

1. Build up some **Abstraction**

2. Provide high **Performance** or **Minimize Overheads**

3. Provide **Protection** between applications, as well as between the OS and applications

4. Provide a high degree of **Reliability**

5. **Energy-Efficiency**

6. **Security**

7. **Mobility**

- An increasingly important as OSes are run on smaller and smaller devices

- Depending on usage, the OS will have different goals and thus likely be implemented in at least slightly different ways

- Many of the principleswe will present on how to build an OS are useful on a range of different devices.

## 2.6 Some History {#history}

### Early Operating Systems: Just Libraries 

- At first OSes were just a set of libraries of commonly-used functions

- For example: instead of programmers writing low-level I/O, the OS would provide APIs making easier time for developers.

- One program ran at a time, controlled by a human operator

- This mode of computing was known as **batch** processing, as a number of jobs were set up and run in "batch" by the operator

### Beyond Libraries: Protection

- OSes began taking a role in managing machines 

- Code ran on behalf of the OS was "special"

- It was special becuase it had control of devices; therefore had to be treated differently due to privacy concerns

- Implementing a **file system** to manage the files as a library makes little sense, and instead, something else was needed...

- The idea of a **system call** was incented, pioneered by the Atlas computing system [K+61, L78].

- Instead of providing OS routines as a library(where you just make a **procedure call** to acces them), the idea here was to add a special pair of hardware instructions and harware state to make the transition into the OS a more formal controlled process

- The key difference between a system call and procedure call is that a system call transfers contol (i.e., jumps) into the Os while simultaneaously raising the **hardware privilege level**

- User applications run in what is reffered to as **user mode**; meaning that the hardware restricts what applications can do

- For example: an application running in user mode can't typically initiate an I/O request to the disk, acces any physical memory page, or send a packet on the netowork

- When a system call is intiated (usually through a special hardware instruction called a **trap**), the hardware transfers contol to a pre-specified **trap handler** (that the OS set up previsouly) and simultaneaously reaises the privilege level to a **kernel mode**

- In kernal mode, the OS has full access to the hardware of the sysem and this can do things like initiate an I/O request or make more memory available to a program 

- When the OS is done dervicing the request, it passes control back to the user via a special **return-from-trap** instruction, which reverts to user mode while simulteaneously passing contorl back to where the application left off

### The Era of Multiprogramming

- Where OS really took off was the era of computing neyond the mainframe, that os the **minicomputer**

- Classic machines like the PDP family from Digital Equipment made computers hugely more affordable; thu instread of having one mainframe per large organizatino, now a smaller collection of people within an organization could likely have their own computer

- **Multiprogramming** became commonplace due to the desier to make betyter use of machine resources.

- Insread of just running one job at a time, the OS would load a number of jobs into memory and seitch rapidly between them, thus improving CPU utilization.

- This switching was important because I/O devices were slow; having a program wait on the CPU while its I/O was being services was a waste of CPU time

- Issues of **memory protection** became important; we wouldn't want one program to be able to access the memory of another program

- One of the major practical advances of the time was the introduction of UNIX operating system

### The Importance of UNIX

- UNIX brought together many great ideas and made a system that was both simple and powerful

- The **shell**, where you type commands, provided primitives such as **pipes** to enable such meta-level programming, and thus it became easy to string together programs to accomplish a bigger task

- For example, to fine lines of a text file that have the word "foo" in them, and then to count how many such lines eist, you would type `grep foo file.txt |wc -l` this using `grep` and `wc` (word count) programs to achieve your task

- UNIX was friendly for programmers and developers alike, also pproviding a compiler for the new **C Programming Language**

- Making it wasy for programmers to write their own programs, as well as share them, made Unix enormously popular

- 